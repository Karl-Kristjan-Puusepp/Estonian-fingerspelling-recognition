{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUyf82TkBADI"
      },
      "source": [
        "# Recognition model of fingerspelling signs in Estonian sign language with Mediapipe and Tensorflow\n",
        "It is recommended you run this file in Google Colab, as some of the used python packages are deprecated and have developed dependency conflicts with Tensorflow. If you wish to run this notebook on your own system, a Linux-based OS is recommended along with a python version between 3.8 and 3.10. The specific versions of Tensorflow and mediapipe-model-maker may need to be tweaked to find a conflict-free result.\n",
        "\n",
        "The default dataset, along with documentation and an interactive web-app can be found at [this repo](https://github.com/Karl-Kristjan-Puusepp/EstonianFingerspellingSigns).\n",
        "\n",
        "This notebook will guide you through\n",
        "1. Importing and preparing the default dataset\n",
        "2. Hand landmark recognition\n",
        "3. Training, evaluating and exporting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is laregly based on [this](https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb#scrollTo=JO1GUwC1_T2x) Google Mediapipe example noteboot that has been fitted for the current use case."
      ],
      "metadata": {
        "id": "8FfGoPFj9vQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "Rln95QCB96px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsKcUJOU6dIP"
      },
      "source": [
        "<a id=\"1\"></a>\n",
        "## 1. Importing and preparing the default dataset\n",
        "First we install the necessary libraries (approx. runtime 2 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oonafdB1g6Ww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527bbacc-7239-4893-9d97-621bc573ef1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m901.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.4/106.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -q mediapipe-model-maker # On Mac systems, a different distribution of Tensorflow is required to be preinstalled\n",
        "!pip install Pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78PP1xiwXBp"
      },
      "source": [
        "We then clone the repo containing the Estonian sign language fingerspelling signs into our project folder.\n",
        "\n",
        "(approx. runtime 2 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCCgf5EzeKhd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Karl-Kristjan-Puusepp/EstonianFingerspellingSigns.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP5XSIDvBXYK"
      },
      "source": [
        "We can check that the dataset has been imported correctly by checking the labels of the dataset. Currently the directory of of the images is set at \"EstonianFingerspellingSigns/data/oneHandedGesturesCropped\". If you wish to use a different dataset, simply change the 'dataset_path' variable. (NOTE: the dataset must include a 'none' folder. This is a requirement of the mediapipe_model_maker.)\n",
        "\n",
        "We also define the path of the reduced dataset used for hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOpkLX8ziKbm"
      },
      "outputs": [],
      "source": [
        "from google.colab import files # Comment out if running locally\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataset_path = \"EstonianFingerspellingSigns/data/oneHandedGestures\" # change if you are using a different dataset\n",
        "\n",
        "# This dataset is only used for hyperparameter tuning and can be omitted\n",
        "# reduced_dataset_path = \"EstonianFingerspellingSigns/data/oneHandedGesturesCroppedReduced\"\n",
        "print(dataset_path)\n",
        "labels = []\n",
        "for i in os.listdir(dataset_path):\n",
        "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
        "    labels.append(i)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnbd4DxRxDaj"
      },
      "source": [
        "To conserve memory, the dataset currently contains only completely unique images. This means the amount of left- and right-handed gestures is unbalanced. To account for this, we mirror every image in both the original and reduced datasets and save it as a copy.\n",
        "\n",
        "(Runtime: Approx 20 sec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYI87Tk5P9XL"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def flip_and_save_images(data_folder):\n",
        "    for root, dirs, files in os.walk(data_folder):\n",
        "        for file in files:\n",
        "            # Check if the file is an image (you can customize the list of valid extensions)\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_path = os.path.join(root, file)\n",
        "                original_image = Image.open(image_path)\n",
        "\n",
        "                # Flip\n",
        "                flipped_image = original_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "                # Append \"_m\" to the original filename (before the file extension)\n",
        "                new_filename = os.path.splitext(file)[0] + \"_m\" + os.path.splitext(file)[1]\n",
        "\n",
        "                save_path = os.path.join(root, new_filename)\n",
        "\n",
        "                # Save the flipped image\n",
        "                flipped_image.save(save_path)\n",
        "                #print(f\"Flipped image {file}\")\n",
        "        print(f\"Label {root} done\")\n",
        "\n",
        "flip_and_save_images(dataset_path)\n",
        "# flip_and_save_images(reduced_dataset_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70GaIP8UB8Q7"
      },
      "source": [
        "\n",
        "## 2. Hand landmark recognition\n",
        "In this step we turn the images into normalised data that can be fed into a machine learning model. For this we use the Mediapipe Hand Gesture Landmark library, which takes in an image and returns a set of 21 landmarks in 3d space, each corresponding to a keypoint or joint in a hand. The default dataset has been culled so that in each image, a hand is always found by the gesture_recognizer.\n",
        "\n",
        "The code below must go through every image in the dataset and perform the recognition. This process takes a while - approx. 1 minute per every 900 images. This scales linearly with more images. In case of the default dataset, the expected runtime is around 14 minutes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU5l1bkViaww"
      },
      "outputs": [],
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)\n",
        "'''\n",
        "reduced_data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=reduced_dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atAlEGlwCOe8"
      },
      "source": [
        "## 3. Training, evaluating and exporting a machine learning model\n",
        "\n",
        "Here, we perform a simplified grid search on the hyperparameters of the dataset to find optimal models, that deliver a good accuracy while not overfitting the data. These measurements are then saved to a pandas dataframe and exported to a csv. This step may be skipped in favor of the code in the next step, with the optimal parameters already inserted as depending on the ranges of values to search, the runtime can be anywhere between 1 - 40h."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UG37cU3KWsV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# Define the CSV file path\n",
        "csv_path = \"gridsearch.csv\"\n",
        "\n",
        "# Create a list to store results for DataFrame\n",
        "results_list = []\n",
        "\n",
        "# Define the hyperparameter ranges\n",
        "epoch_values = [15, 20]\n",
        "batch_size_values = [4, 8]\n",
        "dropout_rate_values = [0.05]\n",
        "layer_widths_values = [[]]\n",
        "\n",
        "# Perform grid search\n",
        "for epochs, batch_size, dropout_rate, layer_widths in itertools.product(epoch_values, batch_size_values, dropout_rate_values, layer_widths_values):\n",
        "    hparams = gesture_recognizer.HParams(epochs=epochs, export_dir=\"exported_model\", batch_size=batch_size)\n",
        "    model_options = gesture_recognizer.ModelOptions(dropout_rate=dropout_rate, layer_widths=layer_widths)\n",
        "    options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
        "\n",
        "    # Create model with current hyperparameters\n",
        "    model = gesture_recognizer.GestureRecognizer.create(\n",
        "        train_data=train_data,\n",
        "        validation_data=validation_data,\n",
        "        options=options\n",
        "    )\n",
        "\n",
        "    loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "    print(f\"Test loss: {loss}, Test accuracy: {acc}\")\n",
        "\n",
        "    # Append results to list for DataFrame\n",
        "    results_list.append([epochs, batch_size, dropout_rate, layer_widths, loss, acc])\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "columns = [\"Epochs\", \"Batch Size\", \"Dropout Rate\", \"Layer Widths\", \"Loss\", \"Accuracy\"]\n",
        "results_df = pd.DataFrame(results_list, columns=columns)\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(results_df)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OdBDr9AKYW6"
      },
      "source": [
        "Training the model with the optimal parameters. Approx runtime: 3min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-2gxGUeinEa"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(epochs=12, export_dir=\"exported_model\", batch_size=16)\n",
        "model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2,layer_widths = [])\n",
        "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMF1PJV1CWEN"
      },
      "source": [
        "Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI5TsBMEk8kX"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz00JSP4CaOE"
      },
      "source": [
        "Exporting and downloading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHWUb0WCmHeQ"
      },
      "outputs": [],
      "source": [
        "  model.export_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JgGHiDN-mQVe",
        "outputId": "b5e305e2-5fe9-4fe2-f459-ed838cec86e7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_40c8da93-81fe-40c2-9a0c-723cee29ee57\", \"gesture_recognizer.task\", 8474869)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}